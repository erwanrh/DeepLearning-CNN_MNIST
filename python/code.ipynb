{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> TP3: NN and CNN with ```pytorch``` </center></h1>\n",
    "\n",
    "The deadline for report submission is Tuesday, December 22th 2020.\n",
    "\n",
    "Note: the goal of this TP is to become familiar with 'Pythor' and to understand how to implement Neural Nets with Pyhtor.\n",
    "\n",
    "We first list the basic function in Pythor and consider a very simple example to understand how Grandient Descent can be implemented. Then we illustrate how set the architecture of neural nets and run it on MNIST dataset. Lastly, we provide an implementation of CNN.\n",
    "\n",
    "As a homework, we propose you implement logistic regression as a neural net and to also to add dropout in CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - cudatoolkit=11.0\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/pytorch/osx-64\n",
      "  - https://conda.anaconda.org/pytorch/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - tqdm\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda               pkgs/main::conda-4.9.2-py38hecd8cb5_0 --> conda-forge::conda-4.9.2-py38h50d1736_0\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch operates with tensors instead of numpy arrays. Almost everything you can do with numpy arrays can be acomplished with pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3592, 0.3340, 0.9200],\n",
      "        [0.8460, 0.8652, 0.5445],\n",
      "        [0.9970, 0.0852, 0.4091]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3) # random tensor of size 3 by 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the result of:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " +\n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]]) \n",
      " = \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# We can operate with pytorch tensors pretty much in the same manner as with numpy arrays\n",
    "x = torch.ones(3,3)\n",
    "y = torch.ones(3,3) * 4\n",
    "z = x + y\n",
    "print(f'This is the result of:\\n {x}\\n +\\n {y} \\n = \\n {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]]) we can look at the last column and 2 rows \n",
      " tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# again we can operate with tensor indexing as if it was a numpy one\n",
    "\n",
    "x = torch.ones(3,3) * 5\n",
    "y = x[-1, :2]\n",
    "print(f'From \\n {x} we can look at the last column and 2 rows \\n {y}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, a lot of ML algorithms can be stated as optimization problems.\n",
    "Let us consider a toy example: imagine that our data is $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^{5}$ is a vector composed of all ones and a label $y = 1$. We would like to find a weight vector $w \\in \\mathbb{R}^{5}$ such that the loss function $L(w) = (y - x^\\top w)^2$ is minimized.\n",
    "\n",
    "Of course, this is a simple least squares on a single observation $(x, y)$ and we can compute the result analytically. But it is a good example to understand what pytorch has to offer.\n",
    "\n",
    "If we are too lazy to compute the analytic expression, we can run the Gradient Descent, which starts from $w_0 = (0, \\ldots, 0)^\\top$ and proceeds as\n",
    "\n",
    "$$w_k = w_{k - 1} - \\eta \\nabla L(w_{k - 1}).$$\n",
    "\n",
    "So the only thing that we need to know is the gradient of the loss function $L$ evaluated at the point $w_{k - 1}$.\n",
    "Here how it is done in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]])\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1)\n",
    "x = torch.ones(1, 5)\n",
    "\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these tensors during the backward pass.\n",
    "w = torch.zeros(5, 1, requires_grad=True) # setting w_0 = (0, ..., 0)^T\n",
    "\n",
    "y_pred = x.mm(w) # inner product of w and x \n",
    "\n",
    "loss = (y - y_pred).pow(2) # squared loss\n",
    "\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all tensors with requires_grad=True.\n",
    "# After this call w.grad will be a tensor holding the gradient\n",
    "# of the loss with respect to w.\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad) # Print the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Assuming that $w_0 = (0, \\ldots, 0)^\\top$ compute on paper $\\nabla L(w_0)$. Do not include the answer to this question into the report. Just make sure you understant what is going on here.\n",
    "\n",
    "Once you made sure that ```w.grad``` indeed stores the value of $\\nabla L(w_0)$. We can implement the Gradient Descent algorithm with only few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/150, Current loss: 0.150094673037529\n",
      "Iteration 20/150, Current loss: 0.018248017877340317\n",
      "Iteration 30/150, Current loss: 0.002218528650701046\n",
      "Iteration 40/150, Current loss: 0.00026972233899869025\n",
      "Iteration 50/150, Current loss: 3.279230440966785e-05\n",
      "Iteration 60/150, Current loss: 3.986556748714065e-06\n",
      "Iteration 70/150, Current loss: 4.846697265747935e-07\n",
      "Iteration 80/150, Current loss: 5.8908199207508005e-08\n",
      "Iteration 90/150, Current loss: 7.173785121494802e-09\n",
      "Iteration 100/150, Current loss: 8.74024408403784e-10\n",
      "Iteration 110/150, Current loss: 1.0756195933936397e-10\n",
      "Iteration 120/150, Current loss: 1.3219647598816664e-11\n",
      "Iteration 130/150, Current loss: 1.566746732351021e-12\n",
      "Iteration 140/150, Current loss: 1.7408297026122455e-13\n",
      "Iteration 150/150, Current loss: 1.2789769243681803e-13\n",
      "Final result: tensor([[0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1)\n",
    "x = torch.ones(1, 5)\n",
    "\n",
    "w = torch.zeros(5, 1, requires_grad=True) # Initialization: w_0 = (0, ..., 0)^T\n",
    "\n",
    "lr = .01 # Learning rate a.k.a. the step size\n",
    "max_iter = 150\n",
    "\n",
    "for k in range(max_iter):\n",
    "    loss = (y - x.mm(w)).pow(2) # forward pass\n",
    "    \n",
    "        \n",
    "    loss.backward() # the backward pass\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad # gradient step\n",
    "        w.grad.zero_() # after performing operation with gradient we need to erase it\n",
    "    \n",
    "    if k % 10 == 9:\n",
    "        print(f'Iteration {k + 1}/{max_iter}, Current loss: {loss.item()}')\n",
    "        \n",
    "print(f'Final result: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Solve the problem $\\min_{w \\in \\mathbb{R}^5}\\, (1 - x^\\top w)^2$ with $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^5$ analytically and compare to the result of the Gradient Descent.\n",
    "\n",
    "**Question:** Recalling the theory of numerical optimization, what is the learning rate ```lr``` that we need to set sto ensure the fastest convergence?\n",
    " \n",
    "**Question:** Explain the connection of ```loss.backward()``` and the backpropagation for feedforward neural nets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will build our neural net. Recall that MNIST is composed of images of size $28 \\times 28$, hence the dimenison of the input is $784$. We have $10$ classes, so the dimension of the output is $10$.\n",
    "\n",
    "In between we will insert $2$ hidden layers and use ReLU as our non-linearity (activation function).\n",
    "The first hidden layer is composed of $128$ neurons and the second one of $64$ neurons.\n",
    "\n",
    "We will not use GPU nor we will consider complicated neural nets in this TP. The goal is to introduce you to the basics without going into too complicated architechtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64],\n",
    "                 output_size=10):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size)\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we defined our neural net we need to train it.\n",
    "The training is going to be performed via Stochastic Gradient Descent evaluated on a mini batch of the data.\n",
    "That is, on the foward stage we will use not a single data point but several ones. In this case we set the size of mini batch equal to $32$.\n",
    "\n",
    "Actually, size of the mini batch, learning rate sizes of hidden layers are all considered as hyperparameters that can be finely tuned (some people even tune random seed, which is absolutely ridiculous). We will not talk about the hypeparameter tuning in this TP, to learn more have a look at https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html .\n",
    "\n",
    "\n",
    "**Important:** We do not require you to perform complicated hyperparameter tuning. This part is beyond the course. However, it is important that you can clearly write an architechture of a nerual net that you consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training consists of gradient steps over mini batch of data\n",
    "def train(model, trainloader, loss, optimizer, epoch, num_epochs):\n",
    "    # We enter train mode. This is useless for the linear model\n",
    "    # but is important for layers such as dropout, batchnorm, ...\n",
    "    model.train()\n",
    "    \n",
    "    loop = tqdm(trainloader)\n",
    "    loop.set_description(f'Training Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    \n",
    "    # We iterate over the mini batches of our data\n",
    "    for inputs, targets in loop:\n",
    "    \n",
    "        # Erase any previously stored gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        outputs = net(inputs) # Forwards stage (prediction with current weights)\n",
    "        loss = criterion(outputs, targets) # loss evaluation\n",
    "        \n",
    "        loss.backward() # Back propagation (evaluate gradients) \n",
    "        \n",
    "        \n",
    "        # Making gradient step on the batch (this function takes care of the gradient step for us)\n",
    "        optimizer.step() \n",
    "        \n",
    "def validation(model, valloader, loss):\n",
    "    # Do not compute gradient, since we do not need it for validation step\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode.\n",
    "        model.eval()\n",
    "        \n",
    "        total = 0 # keep track of currently used samples\n",
    "        running_loss = 0.0 # accumulated loss without averagind\n",
    "        accuracy = 0.0 # accumulated accuracy without averagind (number of correct predictions)\n",
    "        \n",
    "        loop = tqdm(valloader) # This is for the progress bar\n",
    "        loop.set_description('Validation in progress')\n",
    "        \n",
    "        \n",
    "        # We again iterate over the batches of validation data. batch_size does not play any role here\n",
    "        for inputs, targets in loop:\n",
    "            # Run samples through our net\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Total number of used samples\n",
    "            total += inputs.shape[0]\n",
    "\n",
    "            # Multiply loss by the batch size to erase averagind on the batch\n",
    "            running_loss += inputs.shape[0] * loss(outputs, targets).item()\n",
    "            \n",
    "            # how many correct predictions\n",
    "            accuracy += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "            \n",
    "            # set nice progress meassage\n",
    "            loop.set_postfix(val_loss=(running_loss / total), val_acc=(accuracy / total))\n",
    "        return running_loss / total, accuracy / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use again the MNIST dataset. This time we will use the official train/test split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f48447967834d1d99c02abe6cb56e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370249289/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# We download the oficial MNIST train set\n",
    "all_train = datasets.MNIST('data/',\n",
    "                           download=True,\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# We split the whole train set in two parts:\n",
    "# the one that we actually use for training\n",
    "# and the one that we use for validation\n",
    "batch_size = 32 # size of the mini batch\n",
    "num_train = int(0.8 * len(all_train))\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the inputs are torch.Size([32, 1, 28, 28])\n",
      "The number on the image is: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMVUlEQVR4nO3df6jddR3H8derWYgmuqVbw0bm2B9FkMshgZJmlHf+s/VH0UBZpt79kaAYmBiSGKFEPwiF4Mp0K8uYqDhnmHOIFkh4leVmK3WydNtlS/1D94/l9u6P+51dd+/5nuv5/jq77+cDDuec7+ec833z1df9fL7fzzn7OCIEYO77SNcFAGgHYQeSIOxAEoQdSIKwA0mc0ObObHPpH2hYRHim7ZV6dtsjtv9p+xXbN1b5LADN8qDz7LbnSXpJ0tck7ZX0rKQ1EfH3kvfQswMNa6JnP0/SKxHxakT8R9IfJK2q8HkAGlQl7GdKen3K873Ftg+wPWp73PZ4hX0BqKjKBbqZhgrThukRMSZpTGIYD3SpSs++V9KSKc8/JWl/tXIANKVK2J+VtMz2Z2x/TNK3JW2upywAdRt4GB8R79m+RtKfJM2TdHdEvFhbZQBqNfDU20A745wdaFwjX6oBcPwg7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IImBl2wGZmPhwoU92zZs2FD63pUrV5a291uBePfu3T3bli1bVvreuahS2G3vkfSOpMOS3ouIFXUUBaB+dfTsX4mIN2r4HAAN4pwdSKJq2EPS47afsz060wtsj9oetz1ecV8AKqg6jD8/IvbbXihpq+1/RMTTU18QEWOSxiTJdvkVFQCNqdSzR8T+4v6gpIcknVdHUQDqN3DYbZ9s+5SjjyV9XdLOugoDUK8qw/hFkh6yffRzfh8Rj9VSFeaM5cuX92y75JJLSt975MiRSvvuNw+fzcBhj4hXJX2hxloANIipNyAJwg4kQdiBJAg7kARhB5LgJ65o1L333tt1CSjQswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEsyzo5LLL7+8tP3UU09tqZLp7rnnns72PYzo2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCebZUeq0004rbV+zZk1p+7x58+os5wMOHDhQ2r5p06bG9n08omcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYZ0epkZGR0vZ+yy5XsX///tL2yy67rLR99+7ddZZz3Ovbs9u+2/ZB2zunbFtge6vtl4v7+c2WCaCq2QzjN0g69s/7jZK2RcQySduK5wCGWN+wR8TTkt46ZvMqSRuLxxslra65LgA1G/ScfVFETEhSREzYXtjrhbZHJY0OuB8ANWn8Al1EjEkakyTb0fT+AMxs0Km3A7YXS1Jxf7C+kgA0YdCwb5a0tni8VtLD9ZQDoCl9h/G275N0kaTTbe+V9CNJt0vaZPtKSa9J+maTRaI5tkvb58/vblZ13759pe1PPfVUS5XMDX3DHhG9/nWCr9ZcC4AG8XVZIAnCDiRB2IEkCDuQBGEHkuAnrsmdeOKJpe133nlnS5VM99hjj3W277mInh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCefY474YTy/8S33nprS5VMd8cdd5S233bbbS1VkgM9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTz7HHf11VeXtl9//fWN7r9s2eUtW7aUvvfdd9+tu5zU6NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnm2eeABQsW9Gy74oorWqxkuh07dvRse+KJJ1qsBH17dtt32z5oe+eUbbfY3md7e3G7tNkyAVQ1m2H8BkkjM2z/ZUScU9z+WG9ZAOrWN+wR8bSkt1qoBUCDqlygu8b2C8Uwf36vF9ketT1ue7zCvgBUNGjYfy1pqaRzJE1I+nmvF0bEWESsiIgVA+4LQA0GCntEHIiIwxFxRNJdks6rtywAdRso7LYXT3n6DUk7e70WwHDoO89u+z5JF0k63fZeST+SdJHtcySFpD2S1jVYI/p45JFHerade+65LVYy3fbt2zvdP/6vb9gjYs0Mm9c3UAuABvF1WSAJwg4kQdiBJAg7kARhB5LgJ67HgZUrV5a2L126tKVKplu/vnxi5uabb26pEvRDzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDPfhzo9zPVM844o7F9v/nmm6Xt999/f2n74cOH6ywHFdCzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASzLMPgUWLFpW2X3zxxS1VMt3WrVsrtWN40LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBLMsw+Bfr9Hv/DCC1uqBHNZ357d9hLbT9reZftF29cW2xfY3mr75eJ+fvPlAhjUbIbx70n6fkR8VtKXJH3P9uck3ShpW0Qsk7SteA5gSPUNe0RMRMTzxeN3JO2SdKakVZI2Fi/bKGl1U0UCqO5DnbPbPkvSckl/lbQoIiakyT8Ithf2eM+opNFqZQKoatZht/1xSQ9Iui4i3rY9q/dFxJikseIzYpAiAVQ3q6k32x/VZNB/FxEPFpsP2F5ctC+WdLCZEgHUoW/P7skufL2kXRHxiylNmyWtlXR7cf9wIxUmsHo1lzvQvNkM48+XdLmkHba3F9tu0mTIN9m+UtJrkr7ZTIkA6tA37BHxF0m9TtC/Wm85AJrC12WBJAg7kARhB5Ig7EAShB1Igp+4DoG1a9d2tu+I8i81Hjp0qKVK0DR6diAJwg4kQdiBJAg7kARhB5Ig7EAShB1Ignn2IbBt27bS9rPPPruxfT/66KOl7evWrWts32gXPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME8+xC44YYbSttPOumk0vaRkZGebVdddVXpe5955pnSdswd9OxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kIT7/bvhtpdI+o2kT0o6ImksIn5l+xZJV0v6d/HSmyLij30+q3xnACqLiBlXXZ5N2BdLWhwRz9s+RdJzklZL+pakQxHxs9kWQdiB5vUK+2zWZ5+QNFE8fsf2Lkln1lsegKZ9qHN222dJWi7pr8Wma2y/YPtu2/N7vGfU9rjt8UqVAqik7zD+/RfaH5f0lKSfRMSDthdJekNSSPqxJof63+3zGQzjgYYNfM4uSbY/KmmLpD9FxC9maD9L0paI+HyfzyHsQMN6hb3vMN62Ja2XtGtq0IsLd0d9Q9LOqkUCaM5srsZfIOnPknZocupNkm6StEbSOZocxu+RtK64mFf2WfTsQMMqDePrQtiB5g08jAcwNxB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSaHvJ5jck/WvK89OLbcNoWGsb1rokahtUnbV9uldDq79nn7ZzezwiVnRWQIlhrW1Y65KobVBt1cYwHkiCsANJdB32sY73X2ZYaxvWuiRqG1QrtXV6zg6gPV337ABaQtiBJDoJu+0R2/+0/YrtG7uooRfbe2zvsL296/XpijX0DtreOWXbAttbbb9c3M+4xl5Htd1ie19x7LbbvrSj2pbYftL2Ltsv2r622N7psSupq5Xj1vo5u+15kl6S9DVJeyU9K2lNRPy91UJ6sL1H0oqI6PwLGLa/LOmQpN8cXVrL9k8lvRURtxd/KOdHxA+GpLZb9CGX8W6otl7LjH9HHR67Opc/H0QXPft5kl6JiFcj4j+S/iBpVQd1DL2IeFrSW8dsXiVpY/F4oyb/Z2ldj9qGQkRMRMTzxeN3JB1dZrzTY1dSVyu6CPuZkl6f8nyvhmu995D0uO3nbI92XcwMFh1dZqu4X9hxPcfqu4x3m45ZZnxojt0gy59X1UXYZ1qaZpjm/86PiC9KWinpe8VwFbPza0lLNbkG4ISkn3dZTLHM+AOSrouIt7usZaoZ6mrluHUR9r2Slkx5/ilJ+zuoY0YRsb+4PyjpIU2edgyTA0dX0C3uD3Zcz/si4kBEHI6II5LuUofHrlhm/AFJv4uIB4vNnR+7mepq67h1EfZnJS2z/RnbH5P0bUmbO6hjGtsnFxdOZPtkSV/X8C1FvVnS2uLxWkkPd1jLBwzLMt69lhlXx8eu8+XPI6L1m6RLNXlFfrekH3ZRQ4+6zpb0t+L2Yte1SbpPk8O6/2pyRHSlpE9I2ibp5eJ+wRDV9ltNLu39giaDtbij2i7Q5KnhC5K2F7dLuz52JXW1ctz4uiyQBN+gA5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk/gfChK8H2tihXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can iterate over trainloader in the following way\n",
    "for inputs, targets in trainloader:\n",
    "    print(f'Dimensions of the inputs are {inputs.shape}')\n",
    "    plt.imshow(inputs[0][0], cmap='gray', interpolation='none')\n",
    "    print(f'The number on the image is: {targets[0]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of ```inputs``` is $(32, 1, 28, 28)$. The first dimension indicates the size of the mini batch and is controlled by parameter ```batch_size```, the last two parameters are the 2D dimensions of the image and are equal to $28 \\times 28$ in case of the MNIST data. The lonely $1$, staying in the second dimension essentialy reflects the fact that the images are black and white. For instance, if MNIST were colored (there are variants of colored MNIST actually), then we would need $3$ (in case of RGB) colors to represent an image, thus $1$ would be replaed by $3$. \n",
    "\n",
    "**Question:** Run the above block several times. Is it plotting the same number all the time? If not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + training parameters\n",
    "num_epochs = 2 # how many passes over the whole train data\n",
    "input_size = 784 # flattened size of the image\n",
    "hidden_sizes = [128, 64] # sizes of hidden layers\n",
    "output_size = 10 # how many labels we have\n",
    "lr = 0.001 # learning rate\n",
    "momentum = 0.9 # momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "net = SimpleFeedForward(input_size, hidden_sizes, output_size) # Our neural net\n",
    "criterion = nn.CrossEntropyLoss() # Loss function to be optimized\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum) # Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972be0fb8d3f48e7aa3f4b0152ad3c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c85ba670f04180838b551f0865b203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b07e62717e643f0a5a3cf4d29c3fb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038812b7fd08422ea39adb42831db2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a53209b4fba47ed985b3b6de3209ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8973 | Test loss: 0.34896888802051546\n"
     ]
    }
   ],
   "source": [
    "# Let us evaluate our net on the test set that we have never seen!\n",
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Logistic regression via pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above code as an example implement multinomial logistic regression and train it on the same data.\n",
    "For your report include:\n",
    "1. Mathematical description of logistic regression\n",
    "2. Mathematical description of optimization algorithm that you use\n",
    "3. High level idea of how to implement logisitic regression with pytorch\n",
    "4. Report classification accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, output_size),\n",
    "            torch.nn.Softmax(),\n",
    "        )\n",
    "                            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training consists of gradient steps over mini batch of data\n",
    "def train(model, trainloader, loss, optimizer, epoch, num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(trainloader)\n",
    "    loop.set_description(f'Training Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    \n",
    "    # We iterate over the mini batches of our data\n",
    "    for inputs, targets in loop:\n",
    "    \n",
    "        # Erase any previously stored gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        outputs = net(inputs) # Forwards stage (prediction with current weights)\n",
    "        loss = criterion(outputs, targets) # loss evaluation\n",
    "        \n",
    "        loss.backward() # Back propagation (evaluate gradients) \n",
    "        \n",
    "        \n",
    "        # Making gradient step on the batch (this function takes care of the gradient step for us)\n",
    "        optimizer.step() \n",
    "        \n",
    "def validation(model, valloader, loss):\n",
    "    # Do not compute gradient, since we do not need it for validation step\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode.\n",
    "        model.eval()\n",
    "        \n",
    "        total = 0 # keep track of currently used samples\n",
    "        running_loss = 0.0 # accumulated loss without averaging\n",
    "        accuracy = 0.0 # accumulated accuracy without averaging (number of correct predictions)\n",
    "        \n",
    "        loop = tqdm(valloader) # This is for the progress bar\n",
    "        loop.set_description('Validation in progress')\n",
    "        \n",
    "        \n",
    "        # We again iterate over the batches of validation data. batch_size does not play any role here\n",
    "        for inputs, targets in loop:\n",
    "            # Run samples through our net\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Total number of used samples\n",
    "            total += inputs.shape[0]\n",
    "\n",
    "            # Multiply loss by the batch size to erase averagind on the batch\n",
    "            running_loss += inputs.shape[0] * loss(outputs, targets).item()\n",
    "            \n",
    "            # how many correct predictions\n",
    "            accuracy += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "            \n",
    "            # set nice progress meassage\n",
    "            loop.set_postfix(val_loss=(running_loss / total), val_acc=(accuracy / total))\n",
    "        return running_loss / total, accuracy / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# We download the oficial MNIST train set\n",
    "all_train = datasets.MNIST('data/',\n",
    "                           download=True,\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# We split the whole train set in two parts:\n",
    "# the one that we actually use for training\n",
    "# and the one that we use for validation\n",
    "batch_size = 32 # size of the mini batch\n",
    "num_train = int(0.8 * len(all_train))\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + training parameters\n",
    "num_epochs = 2 # how many passes over the whole train data\n",
    "input_size = 784 # flattened size of the image\n",
    "\n",
    "output_size = 10 # how many labels we have\n",
    "lr = 0.01 # learning rate\n",
    "momentum = 0.9 # momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "net = LogisticRegression(input_size, output_size) # Our neural net\n",
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum = momentum) # Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb6c70279074136b9d427bd0d5a568f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d64d23e51e042a496bee693f28501e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b784123a234f508d24a6260618d342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392fb6631c6b4663a0b49606ff61e8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Using downloaded and verified file: data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2c833fd3fd4d1b87b0ae4c2ee7323e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9015 | Test loss: 1.5949760681152343\n"
     ]
    }
   ],
   "source": [
    "# Let us evaluate our net on the test set that we have never seen!\n",
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of CNN: ```nn.Conv2d``` and ```MaxPool2d```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this before starting: https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_flat.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the convolutional layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we instanciate ```nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0)``` it has a parameter ```weight``` which precisely describes the kernel used for our convolution. In the beginning it is initialized randomly, and our goal is to eventually learn its weights (as usual via backpropagation!).\n",
    "Before building our first CNN let us have a look at the kernel and what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.07415688, -0.30996   ],\n",
       "         [-0.20658672, -0.36997014]]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 2x2 kernel (kernel_size=2)\n",
    "# the kernel slides by one step in (x, y) direction (stride=[1, 1])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "conv = nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization.** We will plot the initial image, the kernel, and the resulting image. In order to understand what is going on, the resulting image will be computed in two ways. First of all it will be computed by using ```conv1(image)```. Secondly, we will manually apply the sliding kernel to each $2\\times 2$ window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('2x2 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_convolved = np.zeros((27, 27)) # here we store our result\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        image_convolved[i, j] = np.sum(np_image[i:i+2, j:j+2] * weight) # apply the kernel for each 2x2 window\n",
    "        \n",
    "axs[3].imshow(image_convolved, cmap='gray', interpolation='none')\n",
    "axs[3].set_title('By hand')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem.** Provide 'by hand' implementation of the following kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 4x4 kernel (kernel_size=4)\n",
    "# the kernel slides by 3 step in (x, y) direction (stride=[4, 4])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "conv = nn.Conv2d(1, 1, kernel_size=4, stride=[4, 4], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()\n",
    "\n",
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('4x4 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# PROBLEM: FILL IN THIS PART.\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_conv = np.zeros((7,7))\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        image_conv[i, j]=np.sum(np_image[(4*i):(4*i+4), (4*j):(4*j+4)] * weight)\n",
    "\n",
    "axs[3].imshow(image_conv, cmap='gray', interpolation='none')\n",
    "axs[3].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the pooling layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling is what often used in practice, it amounts to picking only the largest value of a pixel in a given window. In pytorch it is done via ```MaxPool2d(kernel_size=k, stride=s)```, which has two parameters: kernel size and the stride. Note that there are no weights to learn here, so this layer is simply fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kernel_size -- size of the max pool window\n",
    "pool = nn.MaxPool2d(kernel_size=4, stride=[4,4])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Pooling')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "\n",
    "# plot resulting image\n",
    "axs[1].imshow(pool(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[1].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_pooled = np.zeros((7, 7)) # here we store our result\n",
    "for i in range(0, 27, 4):\n",
    "    for j in range(0, 27, 4):\n",
    "        image_pooled[int(i / 4), int(j / 4)] = np.max(np_image[i:i+4, j:j+4]) # max pooling\n",
    "        \n",
    "axs[2].imshow(image_pooled, cmap='gray', interpolation='none')\n",
    "axs[2].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulding a simple ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=[1, 1], padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(14 * 14 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first layer is ```nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)```, the parameters here are chosen in such a way that the size of each output channel remains as $28 \\times 28$. Indeed, setting ```padding = 2``` we augmented our initial image to $32 \\times 32$, then we slide a kernel of size $5 \\times 5$ by $1$ in both $(x, y)$ directions which result in a $28 \\times 28$ output image (and $8$ channels).\n",
    "\n",
    "In general the formula for square images and squared kernels is\n",
    "$$\n",
    "    S_{out} = \\frac{S_{in} - S_{kernel} + 2S_{padding}}{S_{stride}} + 1\n",
    "$$\n",
    "\n",
    "In our case it is\n",
    "\n",
    "$$\n",
    "    S_{out} = \\frac{28 - 5 + 4}{1} + 1 = 28\n",
    "$$\n",
    "\n",
    "Then the output of ```nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)``` goes into ```nn.ReLU()``` our favorite non-linearity and eventually into the pooling layer ```nn.MaxPool2d(kernel_size=2, stride=2)```.\n",
    "The ```nn.ReLU()``` doe not affect the size, hence ```nn.MaxPool2d(kernel_size=2, stride=2)``` receives $8$ channels of $28 \\times 28$ images as computed above.\n",
    "\n",
    "```nn.MaxPool2d(kernel_size=2, stride=2)``` will be applied to each single channel, with ```kernel_size=2, stride=2``` meaning that the output will still have $8$ channels but the images will be halfed in both $(x, y)$ directions. Hence the output of ```nn.MaxPool2d(kernel_size=2, stride=2)``` has $8$ channels with $14 \\times 14$ images.\n",
    "\n",
    "After all this, we will flatten our features and put the into simple ```nn.Linear(14 * 14 * 8, 500)```, where the input size is precisely the output size of ```nn.MaxPool2d(kernel_size=2, stride=2)```, and $500$ stands for the output size of this linear layer.\n",
    "Finally, we apply our favorite nonlinearity to ```nn.Linear(14 * 14 * 8, 500)``` followed by fully connected linear layer ```nn.Linear(500, 10)``` to match the dimension of $10$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549df2c7cb9a4ea8ac9a68c8771fcf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1257659d4cba47dfbf2b9cf345e3caf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff65a078153a409b838d3d7831c03d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ebbf6dfff146f2ab5a147114b77f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b0fbcbb9484ee78fc7d53c572cd0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9796 | Test loss: 0.06116014178171754\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the result here is much better, than in the simple multilayer perceptron. But note, we have actualy trained muuuuuch more parameters here and, at least on my computer, it takes considerably more time.\n",
    "\n",
    "Here you can see the summary of current state of the art results on MNIST: https://www.kaggle.com/c/digit-recognizer/discussion/61480\n",
    "\n",
    "As you see our score barely beats a carefully built random forest or **kNN**! To get extra $0.01$ requires much more fine tuning, which is of course is not the goal here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code for ConvNet and insert Dropout layer (whereever you want).\n",
    "\n",
    "Include in your report:\n",
    "1. High level description of the dropout\n",
    "2. High level description of your architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=[1, 1], padding=2),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(14 * 14 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedbfb10a0ec4a4ca63844c819d78c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf45e94db57430fa3d59bc135130c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c2a1bbca6f42e9a0deb6bc825d7a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5638e6531f4229a9e73199d5cde6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3e5ad5e446417d98301564f77cde8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9753 | Test loss: 0.07293129253424704\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the succesful completion of this TP, we expect you to be able to understand the architectures of NN, CNN.\n",
    "For instance, have a look at the famous AlexNet https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py and see if you can understand its architechture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
